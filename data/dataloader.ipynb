{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from datetime import date, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data_files/'\n",
    "stocks = ['LLY', 'AAPL', 'NEE']\n",
    "years = ['2020', '2021', '2022', '2023', '2024', '2025']\n",
    "mm=['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "params = {'function': 'TIME_SERIES_INTRADAY',\n",
    "          'symbol': 'NEE',\n",
    "          'interval': '1min',\n",
    "          'month': '2025-01',\n",
    "          'outputsize': 'full',\n",
    "          'extended_hours': 'false',\n",
    "          'apikey': os.environ[\"API_KEY\"]\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all returns data\n",
    "for stock in stocks:\n",
    "    start_new_csv = True\n",
    "    params['symbol'] = stock\n",
    "    for year in years:\n",
    "        if year == '2025':\n",
    "            mm = ['01', '02', '03', '04', '05', '06', '07', '08', '09']\n",
    "        else:\n",
    "            mm = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "        for m in mm:\n",
    "            # print(m)\n",
    "            params['month']=year+\"-\"+m\n",
    "            url = \"https://www.alphavantage.co/query?\"\n",
    "            for key, val in params.items():\n",
    "                url+=key\n",
    "                url+='='\n",
    "                url+=val\n",
    "                url+='&'\n",
    "            url = url[:-1]\n",
    "            # print(url)\n",
    "            r = requests.get(url)\n",
    "            data = r.json()\n",
    "            # print(data)\n",
    "            df = pd.DataFrame.from_dict(data['Time Series (1min)'], orient='index')\n",
    "            df = df.sort_index()\n",
    "            # print(df.index)\n",
    "            if start_new_csv:\n",
    "                df1 = df\n",
    "            else:\n",
    "                df1 = pd.concat([df1, df], ignore_index=False)\n",
    "            start_new_csv = False\n",
    "            # print(df1)\n",
    "            # df.to_csv(path + params['symbol'] + params['month'] + '.csv.gz', compression='gzip')\n",
    "    # csv_files = glob.glob(os.path.join(os.getcwd(), f\"*{params['symbol']}*.csv.gz\"))\n",
    "\n",
    "    # combined_df = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "    df1.to_csv(f\"{path + params['symbol']}_combined.csv.gz\", compression='gzip', index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{path}LLY_combined.csv.gz\")\n",
    "    \n",
    "returns = pd.DataFrame({\n",
    "\"time_stamp\": df['Unnamed: 0'],\n",
    "\"log_return\": np.log(1+(df['4. close']-df['1. open'])/df['1. open']),\n",
    "\"volume\": df['5. volume']\n",
    "})\n",
    "\n",
    "moving_average_types = ['volatility', 'daily_returns', 'daily_volu']\n",
    "returns['time_stamp'] = pd.to_datetime(returns['time_stamp'], errors='coerce')\n",
    "\n",
    "daily_volatility = returns.groupby(returns['time_stamp'].dt.floor(\"1D\"))[\"log_return\"].std()\n",
    "volatility_df = pd.DataFrame(daily_volatility).rename(columns={'log_return': 'volatility'})\n",
    "\n",
    "daily_returns = returns.groupby(returns['time_stamp'].dt.floor(\"1D\"))[\"log_return\"].sum()\n",
    "\n",
    "pd.DataFrame(daily_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get daily volatility as target\n",
    "#get moving averages for 1, 5, 22, and 252 days\n",
    "MA = [\"1\", \"5\", \"22\", \"44\"]\n",
    "for stock in stocks:\n",
    "    df = pd.read_csv(f\"{path + stock}_combined.csv.gz\")\n",
    "    \n",
    "    returns = pd.DataFrame({\n",
    "    \"time_stamp\": df['Unnamed: 0'],\n",
    "    \"log_return\": np.log(1+(df['4. close']-df['1. open'])/df['1. open']),\n",
    "    \"volume\": df['5. volume']\n",
    "    })\n",
    "    returns['time_stamp'] = pd.to_datetime(returns['time_stamp'], errors='coerce')\n",
    "\n",
    "    #volatility\n",
    "    # daily_volatility = returns.groupby(returns['time_stamp'].dt.floor(\"1D\"))[\"log_return\"].std()\n",
    "    # volatility_df = pd.DataFrame(daily_volatility).rename(columns={'log_return': 'volatility'})\n",
    "    # volatility_df.to_csv(f\"{path + stock}_DAILY_VOLATILITY.csv.gz\", compression='gzip')\n",
    "\n",
    "    #volatility moving average\n",
    "    # vol_moving_average_df = pd.DataFrame()\n",
    "\n",
    "    # for window in MA:\n",
    "    #     column_name = f'MA_{window}'\n",
    "    #     vol_moving_average_df[column_name] = volatility_df['volatility'].shift(1).rolling(window=int(window)).mean()\n",
    "\n",
    "    # vol_moving_average_df.to_csv(f\"{path + stock}_MOVING_AVERAGE.csv.gz\", compression='gzip')\n",
    "\n",
    "    # #log returns moving average\n",
    "    # daily_returns = pd.Da\n",
    "\n",
    "    moving_average_types = ['volatility', 'log_return', 'volume']\n",
    "    for type in moving_average_types:\n",
    "        if type == \"volatility\":\n",
    "            daily_value = returns.groupby(returns['time_stamp'].dt.floor(\"1D\"))[\"log_return\"].std()\n",
    "            df = pd.DataFrame(daily_value).rename(columns={'log_return': 'volatility'})\n",
    "            df.to_csv(f\"{path + stock}_DAILY_VOLATILITY.csv.gz\", compression='gzip')\n",
    "            \n",
    "        else:\n",
    "            daily_value = returns.groupby(returns['time_stamp'].dt.floor(\"1D\"))[type].sum()\n",
    "            df = pd.DataFrame(daily_value).rename(columns={'log_return': type})\n",
    "        moving_average_df = pd.DataFrame()\n",
    "        for window in MA:\n",
    "            column_name = f'MA_{window}_{type}'\n",
    "            moving_average_df[column_name] = df[type].shift(1).rolling(window=int(window)).mean()\n",
    "        moving_average_df.to_csv(f\"{path+stock}_{type}_MOVING_AVERAGE.csv.gz\", compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_metrics = [\"EARNINGS\", \"INCOME_STATEMENT\", \"BALANCE_SHEET\", \"CASH_FLOW\", \"SHARES_OUTSTANDING\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_range(df, time_name, metric):\n",
    "    df[time_name] = pd.to_datetime(df[time_name])\n",
    "    if metric == \"EARNINGS\":\n",
    "        df = df[df[time_name].dt.year > 2019]\n",
    "    if metric == \"SHARES_OUTSTANDING\":\n",
    "        df = df[df[time_name].date < date(2025, 6, 1)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NA_columns(df, na_column_set):\n",
    "    na_columns = df.columns[df.isna().any()].tolist()\n",
    "    na_column_set.update(na_columns)\n",
    "    return na_column_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_column_set = set()\n",
    "secondary_dfs = dict()\n",
    "for stock in stocks:\n",
    "    # print(stock)\n",
    "    for metric in secondary_metrics:\n",
    "        # print(metric)\n",
    "        url = f'https://www.alphavantage.co/query?function={metric}&symbol={stock}&apikey={os.environ[\"API_KEY\"]}'\n",
    "        # print(url)\n",
    "        r = requests.get(url)\n",
    "        # print(r)\n",
    "        data = r.json()\n",
    "        # print(data)\n",
    "        if metric == \"EARNINGS\":\n",
    "            df = year_range(pd.DataFrame(data['quarterlyEarnings']), 'reportedDate')\n",
    "            df = df[['reportedDate', 'surprise', 'surprisePercentage']]\n",
    "            df = df.rename(columns={'reportedDate': 'time_stamp'})\n",
    "        elif metric == \"SHARES_OUTSTANDING\":\n",
    "            df = year_range(pd.DataFrame(data['data']), 'date')\n",
    "            df = df.rename(columns={'date': 'time_stamp'})\n",
    "        else:\n",
    "            df = year_range(pd.DataFrame(data['annualReports']), 'fiscalDateEnding')\n",
    "            df = df.rename(columns={'fiscalDateEnding': 'time_stamp'})\n",
    "\n",
    "        prefix = f\"{metric}: \"\n",
    "        df.columns = [\n",
    "            f\"{prefix}{col}\" if col != \"time_stamp\" else col\n",
    "            for col in df.columns\n",
    "        ]\n",
    "        df = df.replace([\"None\", \"null\", \"NA\", \"NaN\", \"\"], pd.NA)\n",
    "\n",
    "        na_column_set = get_NA_columns(df, na_column_set)\n",
    "        key_name = f\"{stock}: {metric}\"\n",
    "        secondary_dfs[key_name] = df        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_column_list = na_column_set\n",
    "\n",
    "for stock in stocks:\n",
    "    for metric in secondary_metrics:\n",
    "        key_name = f\"{stock}: {metric}\"\n",
    "        sublist = [s for s in na_column_list if metric in s]\n",
    "        df = secondary_dfs[key_name]\n",
    "        df = df.drop(columns=list(sublist), axis=1)\n",
    "        df.to_csv(f\"{path + stock}_{metric}.csv.gz\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = path+'calendar.txt'\n",
    "cal_dict = {}\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line_number, line in enumerate(file, 1):\n",
    "            value = line.strip()\n",
    "            if value[0]!='2':\n",
    "                key=value\n",
    "                cal_dict[key] = []\n",
    "            else:\n",
    "                yy=value[:4]\n",
    "                mm=value[4:6]\n",
    "                dd=value[6:]\n",
    "                cal_dict[key].append(yy+'-'+mm+'-'+dd)\n",
    "            print(f\"Line {line_number}: {line.strip()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(cal_dict.keys()):\n",
    "    df = pd.DataFrame(cal_dict[key]).rename(columns={0: \"time_stamp\"})\n",
    "    df.to_csv(f\"{path+key}_CALDATES.csv.gz\", compression=\"gzip\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
