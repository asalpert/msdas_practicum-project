{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36d858bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b156c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "db91c16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stock</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>volatility</th>\n",
       "      <th>MA_1_log_return</th>\n",
       "      <th>MA_5_log_return</th>\n",
       "      <th>MA_22_log_return</th>\n",
       "      <th>MA_44_log_return</th>\n",
       "      <th>MA_1_volatility</th>\n",
       "      <th>MA_5_volatility</th>\n",
       "      <th>MA_22_volatility</th>\n",
       "      <th>MA_44_volatility</th>\n",
       "      <th>MA_1_volume</th>\n",
       "      <th>MA_5_volume</th>\n",
       "      <th>MA_22_volume</th>\n",
       "      <th>MA_44_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>1.298290</td>\n",
       "      <td>-0.011603</td>\n",
       "      <td>0.014552</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>4929.640368</td>\n",
       "      <td>5084.752690</td>\n",
       "      <td>4820.954299</td>\n",
       "      <td>4795.661361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-09</td>\n",
       "      <td>3.384507</td>\n",
       "      <td>0.028879</td>\n",
       "      <td>0.008139</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>4994.925196</td>\n",
       "      <td>5039.090881</td>\n",
       "      <td>4831.785040</td>\n",
       "      <td>4800.693345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-10</td>\n",
       "      <td>2.307772</td>\n",
       "      <td>0.008389</td>\n",
       "      <td>-0.000968</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>4924.218329</td>\n",
       "      <td>4990.899754</td>\n",
       "      <td>4841.835622</td>\n",
       "      <td>4804.024129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>1.768501</td>\n",
       "      <td>0.022914</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.003370</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>5101.090690</td>\n",
       "      <td>4985.515579</td>\n",
       "      <td>4862.155744</td>\n",
       "      <td>4812.535856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>5.477530</td>\n",
       "      <td>-0.014601</td>\n",
       "      <td>0.006796</td>\n",
       "      <td>0.003107</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>5070.530383</td>\n",
       "      <td>5004.080993</td>\n",
       "      <td>4878.381499</td>\n",
       "      <td>4822.092545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664980</th>\n",
       "      <td>178</td>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>-0.073667</td>\n",
       "      <td>-0.006592</td>\n",
       "      <td>-0.000556</td>\n",
       "      <td>0.005221</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>3057.298140</td>\n",
       "      <td>2950.713931</td>\n",
       "      <td>2837.638705</td>\n",
       "      <td>2858.451023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664981</th>\n",
       "      <td>178</td>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>0.018076</td>\n",
       "      <td>-0.004841</td>\n",
       "      <td>-0.004158</td>\n",
       "      <td>0.002922</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>3106.709103</td>\n",
       "      <td>2985.308763</td>\n",
       "      <td>2848.354836</td>\n",
       "      <td>2871.871304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664982</th>\n",
       "      <td>178</td>\n",
       "      <td>2025-09-26</td>\n",
       "      <td>-0.180991</td>\n",
       "      <td>-0.024329</td>\n",
       "      <td>-0.010258</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>3226.019775</td>\n",
       "      <td>3090.855058</td>\n",
       "      <td>2873.852732</td>\n",
       "      <td>2882.683179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664983</th>\n",
       "      <td>178</td>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>0.010319</td>\n",
       "      <td>0.009607</td>\n",
       "      <td>-0.006998</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>3100.716181</td>\n",
       "      <td>3121.269799</td>\n",
       "      <td>2897.020777</td>\n",
       "      <td>2891.673508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664984</th>\n",
       "      <td>178</td>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>-0.048870</td>\n",
       "      <td>-0.009611</td>\n",
       "      <td>-0.007153</td>\n",
       "      <td>0.002517</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>3062.258567</td>\n",
       "      <td>3110.600353</td>\n",
       "      <td>2908.873887</td>\n",
       "      <td>2902.983226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>664985 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Stock time_stamp  volatility  MA_1_log_return  MA_5_log_return  \\\n",
       "0           1 2020-03-06    1.298290        -0.011603         0.014552   \n",
       "1           1 2020-03-09    3.384507         0.028879         0.008139   \n",
       "2           1 2020-03-10    2.307772         0.008389        -0.000968   \n",
       "3           1 2020-03-11    1.768501         0.022914         0.012900   \n",
       "4           1 2020-03-12    5.477530        -0.014601         0.006796   \n",
       "...       ...        ...         ...              ...              ...   \n",
       "664980    178 2025-09-24   -0.073667        -0.006592        -0.000556   \n",
       "664981    178 2025-09-25    0.018076        -0.004841        -0.004158   \n",
       "664982    178 2025-09-26   -0.180991        -0.024329        -0.010258   \n",
       "664983    178 2025-09-29    0.010319         0.009607        -0.006998   \n",
       "664984    178 2025-09-30   -0.048870        -0.009611        -0.007153   \n",
       "\n",
       "        MA_22_log_return  MA_44_log_return  MA_1_volatility  MA_5_volatility  \\\n",
       "0               0.000846          0.000877         0.001197         0.001936   \n",
       "1               0.001962          0.001219         0.001461         0.001623   \n",
       "2               0.002626          0.001462         0.002529         0.001815   \n",
       "3               0.003370          0.001635         0.001978         0.001687   \n",
       "4               0.003107          0.001434         0.001702         0.001773   \n",
       "...                  ...               ...              ...              ...   \n",
       "664980          0.005221          0.001720         0.000687         0.000814   \n",
       "664981          0.002922          0.001722         0.000760         0.000745   \n",
       "664982          0.001936          0.001316         0.000807         0.000762   \n",
       "664983          0.002705          0.000942         0.000705         0.000743   \n",
       "664984          0.002517          0.000774         0.000803         0.000752   \n",
       "\n",
       "        MA_22_volatility  MA_44_volatility  MA_1_volume  MA_5_volume  \\\n",
       "0               0.001052          0.000802  4929.640368  5084.752690   \n",
       "1               0.001093          0.000825  4994.925196  5039.090881   \n",
       "2               0.001180          0.000869  4924.218329  4990.899754   \n",
       "3               0.001248          0.000904  5101.090690  4985.515579   \n",
       "4               0.001298          0.000933  5070.530383  5004.080993   \n",
       "...                  ...               ...          ...          ...   \n",
       "664980          0.000833          0.000873  3057.298140  2950.713931   \n",
       "664981          0.000823          0.000874  3106.709103  2985.308763   \n",
       "664982          0.000830          0.000872  3226.019775  3090.855058   \n",
       "664983          0.000834          0.000875  3100.716181  3121.269799   \n",
       "664984          0.000834          0.000877  3062.258567  3110.600353   \n",
       "\n",
       "        MA_22_volume  MA_44_volume  \n",
       "0        4820.954299   4795.661361  \n",
       "1        4831.785040   4800.693345  \n",
       "2        4841.835622   4804.024129  \n",
       "3        4862.155744   4812.535856  \n",
       "4        4878.381499   4822.092545  \n",
       "...              ...           ...  \n",
       "664980   2837.638705   2858.451023  \n",
       "664981   2848.354836   2871.871304  \n",
       "664982   2873.852732   2882.683179  \n",
       "664983   2897.020777   2891.673508  \n",
       "664984   2908.873887   2902.983226  \n",
       "\n",
       "[664985 rows x 15 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"final_dataset_tech.csv.gz\").drop(\"Unnamed: 0\", axis=1)\n",
    "df['time_stamp'] = pd.to_datetime(df['time_stamp'])\n",
    "df[\"volatility\"] = (df[\"volatility\"] - df['volatility'].mean()) / df[\"volatility\"].std()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf14fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize day of year/365\n",
    "#do not change the year\n",
    "#Normalization z-scale normalization for each of the features,\n",
    "#Normalization in volatiity would not give big boost\n",
    "#with high volatility, look at residuals of volatility and put a time based\n",
    "    #Actual - predicted = residuals (apply for both of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "80f11fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_years = [2020,2021,2022,2023,2024]\n",
    "training_dataset = df[df[\"time_stamp\"].dt.year.isin(testing_years)]\n",
    "testing_dataset = df[df[\"time_stamp\"].dt.year==2025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ac65585f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vb/pdc3w9394cvftrnpc2zrn3dc0000gn/T/ipykernel_60075/2529267694.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_df['time_stamp'] = X_train_df[\"time_stamp\"].astype('int64') // 10**9\n",
      "/var/folders/vb/pdc3w9394cvftrnpc2zrn3dc0000gn/T/ipykernel_60075/2529267694.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_df[X_train_df.columns] = (X_train_df[X_train_df.columns] - X_train_df[X_train_df.columns].mean()) / X_train_df[X_train_df.columns].std()\n"
     ]
    }
   ],
   "source": [
    "y_train = training_dataset[\"volatility\"].values\n",
    "X_train_df = training_dataset.loc[:, training_dataset.columns != 'volatility']\n",
    "X_train_df['time_stamp'] = X_train_df[\"time_stamp\"].astype('int64') // 10**9\n",
    "X_train_df[X_train_df.columns] = (X_train_df[X_train_df.columns] - X_train_df[X_train_df.columns].mean()) / X_train_df[X_train_df.columns].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "14e2cc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vb/pdc3w9394cvftrnpc2zrn3dc0000gn/T/ipykernel_60075/1241999720.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_df['time_stamp'] = X_test_df[\"time_stamp\"].astype('int64') // 10**9\n",
      "/var/folders/vb/pdc3w9394cvftrnpc2zrn3dc0000gn/T/ipykernel_60075/1241999720.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_df[X_test_df.columns] = (X_test_df[X_test_df.columns] - X_test_df[X_test_df.columns].mean()) / X_test_df[X_test_df.columns].std()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stock</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>MA_1_log_return</th>\n",
       "      <th>MA_5_log_return</th>\n",
       "      <th>MA_22_log_return</th>\n",
       "      <th>MA_44_log_return</th>\n",
       "      <th>MA_1_volatility</th>\n",
       "      <th>MA_5_volatility</th>\n",
       "      <th>MA_22_volatility</th>\n",
       "      <th>MA_44_volatility</th>\n",
       "      <th>MA_1_volume</th>\n",
       "      <th>MA_5_volume</th>\n",
       "      <th>MA_22_volume</th>\n",
       "      <th>MA_44_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>-1.720970</td>\n",
       "      <td>-1.743371</td>\n",
       "      <td>-0.388356</td>\n",
       "      <td>-3.298895</td>\n",
       "      <td>-0.473679</td>\n",
       "      <td>-0.021692</td>\n",
       "      <td>-0.702458</td>\n",
       "      <td>2.758504</td>\n",
       "      <td>-0.039946</td>\n",
       "      <td>-0.531304</td>\n",
       "      <td>1.627598</td>\n",
       "      <td>1.457961</td>\n",
       "      <td>1.694006</td>\n",
       "      <td>1.782305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>-1.720970</td>\n",
       "      <td>-1.730590</td>\n",
       "      <td>-1.281828</td>\n",
       "      <td>-0.787459</td>\n",
       "      <td>-0.849046</td>\n",
       "      <td>-0.195713</td>\n",
       "      <td>-0.225206</td>\n",
       "      <td>-0.708400</td>\n",
       "      <td>0.009310</td>\n",
       "      <td>-0.503653</td>\n",
       "      <td>1.991296</td>\n",
       "      <td>1.760766</td>\n",
       "      <td>1.756487</td>\n",
       "      <td>1.792181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>-1.720970</td>\n",
       "      <td>-1.692246</td>\n",
       "      <td>-0.207194</td>\n",
       "      <td>-0.956437</td>\n",
       "      <td>-0.973556</td>\n",
       "      <td>-0.141889</td>\n",
       "      <td>-0.529352</td>\n",
       "      <td>-0.622959</td>\n",
       "      <td>0.015906</td>\n",
       "      <td>-0.497836</td>\n",
       "      <td>1.775048</td>\n",
       "      <td>1.808723</td>\n",
       "      <td>1.759480</td>\n",
       "      <td>1.792399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>-1.720970</td>\n",
       "      <td>-1.679464</td>\n",
       "      <td>0.069239</td>\n",
       "      <td>-0.772709</td>\n",
       "      <td>-1.062332</td>\n",
       "      <td>-0.025898</td>\n",
       "      <td>-0.634913</td>\n",
       "      <td>-0.637026</td>\n",
       "      <td>0.033718</td>\n",
       "      <td>-0.507455</td>\n",
       "      <td>1.698163</td>\n",
       "      <td>1.798733</td>\n",
       "      <td>1.759278</td>\n",
       "      <td>1.788222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>-1.720970</td>\n",
       "      <td>-1.666683</td>\n",
       "      <td>-0.217879</td>\n",
       "      <td>-0.897464</td>\n",
       "      <td>-1.089894</td>\n",
       "      <td>-0.120422</td>\n",
       "      <td>-0.578093</td>\n",
       "      <td>-0.645530</td>\n",
       "      <td>0.054632</td>\n",
       "      <td>-0.531669</td>\n",
       "      <td>1.742184</td>\n",
       "      <td>1.818094</td>\n",
       "      <td>1.761337</td>\n",
       "      <td>1.782593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664980</th>\n",
       "      <td>-0.430199</td>\n",
       "      <td>1.643689</td>\n",
       "      <td>-0.365934</td>\n",
       "      <td>-0.080497</td>\n",
       "      <td>1.170552</td>\n",
       "      <td>0.518256</td>\n",
       "      <td>-0.217738</td>\n",
       "      <td>0.087085</td>\n",
       "      <td>0.167795</td>\n",
       "      <td>0.323323</td>\n",
       "      <td>0.131684</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.145059</td>\n",
       "      <td>-0.109051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664981</th>\n",
       "      <td>-0.430199</td>\n",
       "      <td>1.656470</td>\n",
       "      <td>-0.270045</td>\n",
       "      <td>-0.516262</td>\n",
       "      <td>0.648352</td>\n",
       "      <td>0.518857</td>\n",
       "      <td>-0.052806</td>\n",
       "      <td>-0.102672</td>\n",
       "      <td>0.131811</td>\n",
       "      <td>0.328304</td>\n",
       "      <td>0.196304</td>\n",
       "      <td>0.044353</td>\n",
       "      <td>-0.130377</td>\n",
       "      <td>-0.090496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664982</th>\n",
       "      <td>-0.430199</td>\n",
       "      <td>1.669252</td>\n",
       "      <td>-1.337486</td>\n",
       "      <td>-1.254369</td>\n",
       "      <td>0.424380</td>\n",
       "      <td>0.393943</td>\n",
       "      <td>0.054165</td>\n",
       "      <td>-0.055959</td>\n",
       "      <td>0.159048</td>\n",
       "      <td>0.319442</td>\n",
       "      <td>0.352339</td>\n",
       "      <td>0.185932</td>\n",
       "      <td>-0.095445</td>\n",
       "      <td>-0.075547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664983</th>\n",
       "      <td>-0.430199</td>\n",
       "      <td>1.707596</td>\n",
       "      <td>0.521357</td>\n",
       "      <td>-0.859872</td>\n",
       "      <td>0.599022</td>\n",
       "      <td>0.278884</td>\n",
       "      <td>-0.177944</td>\n",
       "      <td>-0.109994</td>\n",
       "      <td>0.170224</td>\n",
       "      <td>0.330988</td>\n",
       "      <td>0.188467</td>\n",
       "      <td>0.226731</td>\n",
       "      <td>-0.063704</td>\n",
       "      <td>-0.063117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664984</th>\n",
       "      <td>-0.430199</td>\n",
       "      <td>1.720377</td>\n",
       "      <td>-0.531341</td>\n",
       "      <td>-0.878657</td>\n",
       "      <td>0.556336</td>\n",
       "      <td>0.227328</td>\n",
       "      <td>0.045121</td>\n",
       "      <td>-0.083436</td>\n",
       "      <td>0.173298</td>\n",
       "      <td>0.340694</td>\n",
       "      <td>0.138172</td>\n",
       "      <td>0.212419</td>\n",
       "      <td>-0.047465</td>\n",
       "      <td>-0.047481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88339 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Stock  time_stamp  MA_1_log_return  MA_5_log_return  \\\n",
       "1214   -1.720970   -1.743371        -0.388356        -3.298895   \n",
       "1215   -1.720970   -1.730590        -1.281828        -0.787459   \n",
       "1216   -1.720970   -1.692246        -0.207194        -0.956437   \n",
       "1217   -1.720970   -1.679464         0.069239        -0.772709   \n",
       "1218   -1.720970   -1.666683        -0.217879        -0.897464   \n",
       "...          ...         ...              ...              ...   \n",
       "664980 -0.430199    1.643689        -0.365934        -0.080497   \n",
       "664981 -0.430199    1.656470        -0.270045        -0.516262   \n",
       "664982 -0.430199    1.669252        -1.337486        -1.254369   \n",
       "664983 -0.430199    1.707596         0.521357        -0.859872   \n",
       "664984 -0.430199    1.720377        -0.531341        -0.878657   \n",
       "\n",
       "        MA_22_log_return  MA_44_log_return  MA_1_volatility  MA_5_volatility  \\\n",
       "1214           -0.473679         -0.021692        -0.702458         2.758504   \n",
       "1215           -0.849046         -0.195713        -0.225206        -0.708400   \n",
       "1216           -0.973556         -0.141889        -0.529352        -0.622959   \n",
       "1217           -1.062332         -0.025898        -0.634913        -0.637026   \n",
       "1218           -1.089894         -0.120422        -0.578093        -0.645530   \n",
       "...                  ...               ...              ...              ...   \n",
       "664980          1.170552          0.518256        -0.217738         0.087085   \n",
       "664981          0.648352          0.518857        -0.052806        -0.102672   \n",
       "664982          0.424380          0.393943         0.054165        -0.055959   \n",
       "664983          0.599022          0.278884        -0.177944        -0.109994   \n",
       "664984          0.556336          0.227328         0.045121        -0.083436   \n",
       "\n",
       "        MA_22_volatility  MA_44_volatility  MA_1_volume  MA_5_volume  \\\n",
       "1214           -0.039946         -0.531304     1.627598     1.457961   \n",
       "1215            0.009310         -0.503653     1.991296     1.760766   \n",
       "1216            0.015906         -0.497836     1.775048     1.808723   \n",
       "1217            0.033718         -0.507455     1.698163     1.798733   \n",
       "1218            0.054632         -0.531669     1.742184     1.818094   \n",
       "...                  ...               ...          ...          ...   \n",
       "664980          0.167795          0.323323     0.131684    -0.002052   \n",
       "664981          0.131811          0.328304     0.196304     0.044353   \n",
       "664982          0.159048          0.319442     0.352339     0.185932   \n",
       "664983          0.170224          0.330988     0.188467     0.226731   \n",
       "664984          0.173298          0.340694     0.138172     0.212419   \n",
       "\n",
       "        MA_22_volume  MA_44_volume  \n",
       "1214        1.694006      1.782305  \n",
       "1215        1.756487      1.792181  \n",
       "1216        1.759480      1.792399  \n",
       "1217        1.759278      1.788222  \n",
       "1218        1.761337      1.782593  \n",
       "...              ...           ...  \n",
       "664980     -0.145059     -0.109051  \n",
       "664981     -0.130377     -0.090496  \n",
       "664982     -0.095445     -0.075547  \n",
       "664983     -0.063704     -0.063117  \n",
       "664984     -0.047465     -0.047481  \n",
       "\n",
       "[88339 rows x 14 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = testing_dataset[\"volatility\"].values\n",
    "X_test_df = testing_dataset.loc[:, testing_dataset.columns != 'volatility']\n",
    "X_test_df['time_stamp'] = X_test_df[\"time_stamp\"].astype('int64') // 10**9\n",
    "X_test_df[X_test_df.columns] = (X_test_df[X_test_df.columns] - X_test_df[X_test_df.columns].mean()) / X_test_df[X_test_df.columns].std()\n",
    "X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b04492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576646, 16)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"training_dataset_tech.csv\")\n",
    "print(df.shape)\n",
    "y_train = df['volatility'].values\n",
    "X_train = df.loc[:, df.columns != 'volatility'].values\n",
    "\n",
    "X_tensor_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_tensor_train = torch.tensor(y_train, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6876f461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0005217733928955276)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31041010",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"testing_dataset_tech.csv\")\n",
    "y_test = df['volatility'].values\n",
    "X_test = df.loc[:, df.columns != 'volatility'].values\n",
    "\n",
    "X_tensor_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_tensor_test = torch.tensor(y_test, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a216ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out train test split and fine tuning\n",
    "training_data = TensorDataset(X_tensor_train, y_tensor_train)\n",
    "testing_data = TensorDataset(X_tensor_test, y_tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f1e1acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 15])\n",
      "Shape of y: torch.Size([64]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa78a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=15, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(15, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e426ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x, h0=None, c0=None):\n",
    "        if h0 is None or c0 is None:\n",
    "            h0 = torch.zeros(self.layer_dim, x.size(\n",
    "                0), self.hidden_dim).to(x.device)\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hn, cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b529d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_dim=)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f5978a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbbb3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            preds.append(pred.cpu())\n",
    "            targets.append(y.cpu())\n",
    "    test_loss /= num_batches\n",
    "    pred_np = torch.cat(preds).numpy()\n",
    "    y_np = torch.cat(targets).numpy()\n",
    "\n",
    "    r_squared = r2_score(y_np, pred_np)\n",
    "    print(f\"Test Error: \\n R-Squared: {r_squared}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "868b0ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.003202  [   64/576646]\n",
      "loss: 0.001526  [64064/576646]\n",
      "loss: 0.000468  [128064/576646]\n",
      "loss: 0.000068  [192064/576646]\n",
      "loss: 0.000002  [256064/576646]\n",
      "loss: 0.000000  [320064/576646]\n",
      "loss: 0.000000  [384064/576646]\n",
      "loss: 0.000000  [448064/576646]\n",
      "loss: 0.000000  [512064/576646]\n",
      "loss: 0.000000  [576064/576646]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.000001  [   64/576646]\n",
      "loss: 0.000000  [64064/576646]\n",
      "loss: 0.000000  [128064/576646]\n",
      "loss: 0.000000  [192064/576646]\n",
      "loss: 0.000000  [256064/576646]\n",
      "loss: 0.000000  [320064/576646]\n",
      "loss: 0.000000  [384064/576646]\n",
      "loss: 0.000000  [448064/576646]\n",
      "loss: 0.000000  [512064/576646]\n",
      "loss: 0.000000  [576064/576646]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.000001  [   64/576646]\n",
      "loss: 0.000000  [64064/576646]\n",
      "loss: 0.000000  [128064/576646]\n",
      "loss: 0.000000  [192064/576646]\n",
      "loss: 0.000000  [256064/576646]\n",
      "loss: 0.000000  [320064/576646]\n",
      "loss: 0.000001  [384064/576646]\n",
      "loss: 0.000000  [448064/576646]\n",
      "loss: 0.000000  [512064/576646]\n",
      "loss: 0.000000  [576064/576646]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.000001  [   64/576646]\n",
      "loss: 0.000000  [64064/576646]\n",
      "loss: 0.000000  [128064/576646]\n",
      "loss: 0.000000  [192064/576646]\n",
      "loss: 0.000000  [256064/576646]\n",
      "loss: 0.000000  [320064/576646]\n",
      "loss: 0.000001  [384064/576646]\n",
      "loss: 0.000000  [448064/576646]\n",
      "loss: 0.000000  [512064/576646]\n",
      "loss: 0.000000  [576064/576646]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.000001  [   64/576646]\n",
      "loss: 0.000000  [64064/576646]\n",
      "loss: 0.000000  [128064/576646]\n",
      "loss: 0.000000  [192064/576646]\n",
      "loss: 0.000000  [256064/576646]\n",
      "loss: 0.000000  [320064/576646]\n",
      "loss: 0.000001  [384064/576646]\n",
      "loss: 0.000000  [448064/576646]\n",
      "loss: 0.000000  [512064/576646]\n",
      "loss: 0.000000  [576064/576646]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.000001  [   64/576646]\n",
      "loss: 0.000000  [64064/576646]\n",
      "loss: 0.000000  [128064/576646]\n",
      "loss: 0.000000  [192064/576646]\n",
      "loss: 0.000000  [256064/576646]\n",
      "loss: 0.000000  [320064/576646]\n",
      "loss: 0.000001  [384064/576646]\n",
      "loss: 0.000000  [448064/576646]\n",
      "loss: 0.000000  [512064/576646]\n",
      "loss: 0.000000  [576064/576646]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.000001  [   64/576646]\n",
      "loss: 0.000000  [64064/576646]\n",
      "loss: 0.000000  [128064/576646]\n",
      "loss: 0.000000  [192064/576646]\n",
      "loss: 0.000000  [256064/576646]\n",
      "loss: 0.000000  [320064/576646]\n",
      "loss: 0.000001  [384064/576646]\n",
      "loss: 0.000000  [448064/576646]\n",
      "loss: 0.000000  [512064/576646]\n",
      "loss: 0.000000  [576064/576646]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.000001  [   64/576646]\n",
      "loss: 0.000000  [64064/576646]\n",
      "loss: 0.000000  [128064/576646]\n",
      "loss: 0.000000  [192064/576646]\n",
      "loss: 0.000000  [256064/576646]\n",
      "loss: 0.000000  [320064/576646]\n",
      "loss: 0.000001  [384064/576646]\n",
      "loss: 0.000000  [448064/576646]\n",
      "loss: 0.000000  [512064/576646]\n",
      "loss: 0.000000  [576064/576646]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.000001  [   64/576646]\n",
      "loss: 0.000000  [64064/576646]\n",
      "loss: 0.000000  [128064/576646]\n",
      "loss: 0.000000  [192064/576646]\n",
      "loss: 0.000000  [256064/576646]\n",
      "loss: 0.000000  [320064/576646]\n",
      "loss: 0.000001  [384064/576646]\n",
      "loss: 0.000000  [448064/576646]\n",
      "loss: 0.000000  [512064/576646]\n",
      "loss: 0.000000  [576064/576646]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.000001  [   64/576646]\n",
      "loss: 0.000000  [64064/576646]\n",
      "loss: 0.000000  [128064/576646]\n",
      "loss: 0.000000  [192064/576646]\n",
      "loss: 0.000000  [256064/576646]\n",
      "loss: 0.000000  [320064/576646]\n",
      "loss: 0.000001  [384064/576646]\n",
      "loss: 0.000000  [448064/576646]\n",
      "loss: 0.000000  [512064/576646]\n",
      "loss: 0.000000  [576064/576646]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91e3363a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " R-Squared: -0.6514965295791626%, Avg loss: 0.000000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7220e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8320d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
